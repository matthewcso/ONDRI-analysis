{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_folder = 'outputs/metrics_ondri_noncsf/'\n",
    "metric_df = []\n",
    "seg_files = os.listdir(seg_folder)\n",
    "for file in seg_files:\n",
    "    metric_df.append(pd.read_csv(seg_folder + file))\n",
    "\n",
    "metric_df = pd.concat(metric_df)\n",
    "metric_df['SUBJECT'] = [x.split('.')[:-1][0] for x in seg_files]\n",
    "\n",
    "metric_df = metric_df.dropna(axis=1, how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_cols = metric_df.columns[metric_df.columns.str.contains('MII') | metric_df.columns.str.contains('mean')]\n",
    "metric_df = metric_df.drop(delete_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvox_cols = metric_df.columns[metric_df.columns.str.contains('nvox')]\n",
    "metric_df[nvox_cols] = np.asarray(metric_df[nvox_cols])/np.expand_dims(metric_df['TBV-voxel'], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_df = pd.read_csv('data/summary/ONDRI_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(clinical_df[['SUBJECT', 'COHORT']], metric_df, on='SUBJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.columns = [x.replace('Left', 'Temp') for x in merged.columns]\n",
    "merged.columns = [x.replace('Right', 'Left') for x in merged.columns]\n",
    "merged.columns = [x.replace('Temp', 'Right') for x in merged.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('outputs/analysis'):\n",
    "    os.makedirs('outputs/analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n"
     ]
    }
   ],
   "source": [
    "# train models\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import  recall_score, roc_auc_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix as confusion_matrix_fn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "import shap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for version in  'texture', 'both':\n",
    "    comparisons = [('HC', 'ADMCI'), ('HC', 'FTD'), ('HC', 'PD'), ('HC', 'ALS'), ('ADMCI', 'FTD'), ('FTD', 'ALS'), ('ADMCI', 'PD')]\n",
    "    classification_accuracy_df = []\n",
    "\n",
    "    for comparison in comparisons:\n",
    "        labelmap = {comparison[0]: 0, comparison[1]: 1}\n",
    "\n",
    "        merged_select = merged[merged['COHORT'].isin(list(labelmap.keys()))]\n",
    "        features = merged_select[list(filter(lambda x: x not in [ 'SUBJECT', 'COHORT'], merged_select.columns))]\n",
    "        features = features.dropna(axis=1, how='any')\n",
    "        \n",
    "        if version == 'vol':\n",
    "            vol_cols = list(filter(lambda x: 'nvox' in x, features.columns))\n",
    "            features = features[vol_cols]\n",
    "        elif version == 'texture':\n",
    "            mii_cols = list(filter(lambda x: 'MAD' in x, features.columns)) \n",
    "            features = features[mii_cols]\n",
    "        #features = features.drop(columns=['TBV-voxel'])\n",
    "\n",
    "        labels = merged['COHORT']\n",
    "        labels = merged_select['COHORT'].map(labelmap)\n",
    "\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=49)\n",
    "\n",
    "        val_predictions_lr = np.zeros((labels.shape[0], 2))\n",
    "        val_predictions_rf = np.zeros((labels.shape[0], 2))\n",
    "        models_lr = []\n",
    "        shap_lr = []\n",
    "        models_rf =[]\n",
    "        shap_rf = []\n",
    "\n",
    "        for train_idx, val_idx in kf.split(features):\n",
    "            train_features = features.iloc[train_idx]\n",
    "            train_labels = labels.iloc[train_idx]\n",
    "            val_features = features.iloc[val_idx]\n",
    "            val_labels = labels.iloc[val_idx]\n",
    "\n",
    "            # ---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            # Random Forest training\n",
    "            model = BalancedRandomForestClassifier(random_state=0)\n",
    "            model.fit(train_features, train_labels)\n",
    "\n",
    "            explainer = shap.TreeExplainer(model, data=val_features)\n",
    "            shap_values = explainer.shap_values(val_features, check_additivity=False)\n",
    "\n",
    "            shap_rf.append(shap_values)\n",
    "            models_rf.append(model)\n",
    "            val_predictions_rf[val_idx] = model.predict_proba(val_features)\n",
    "\n",
    "\n",
    "        coef_df = pd.DataFrame()\n",
    "        coef_df['feature'] = features.columns\n",
    "        mean_imps = np.mean(np.array([model.feature_importances_ for model in models_rf]), axis=0)\n",
    "        mean_shaps = np.mean(np.abs(np.concatenate([shap_values[1] for shap_values in shap_rf])), axis=0)\n",
    "\n",
    "        coef_df['shap'] = mean_shaps\n",
    "        coef_df['imps'] = mean_imps\n",
    "        coef_df['mean_percent'] = [(np.mean(features[x][labels==1])/np.mean(features[x][labels==0])) for x in coef_df['feature']]\n",
    "        coef_df['p'] = [ttest_ind(features[x][labels==0], features[x][labels==1]).pvalue for x in coef_df['feature']]\n",
    "        coef_df = coef_df.sort_values(by='shap', ascending=False)\n",
    "        coef_df.to_csv('outputs/analysis/%s_vs_%s_%s_noncsf.csv' % (comparison[0], comparison[1], version), index=False)\n",
    "\n",
    "\n",
    "        confusion_matrix = confusion_matrix_fn(labels, np.argmax(val_predictions_rf, axis=-1))\n",
    "\n",
    "        recall_0 = confusion_matrix[0, 0]/ (confusion_matrix[0, 0] + confusion_matrix[0, 1]) # (yes this agrees with sklearn confusion_matrix)\n",
    "        recall_1 = confusion_matrix[1, 1]/ (confusion_matrix[1, 0] + confusion_matrix[1, 1])\n",
    "\n",
    "        classification_accuracy_df.append(pd.Series({'comparison': '%s vs %s' % (comparison[0], comparison[1]), \n",
    "                                                    'auc': roc_auc_score(labels, val_predictions_rf[:, 1]),\n",
    "                                                    'Recall 0': recall_0,\n",
    "                                                    'Recall 1': recall_1,\n",
    "                                                    })) \n",
    "                                                    \n",
    "                                                                    \n",
    "    classification_accuracy_df = pd.DataFrame(classification_accuracy_df)\n",
    "\n",
    "    classification_accuracy_df['Recall 0'] = np.round(classification_accuracy_df['Recall 0'], 2)\n",
    "    classification_accuracy_df['Recall 1'] = np.round(classification_accuracy_df['Recall 1'], 2)\n",
    "    classification_accuracy_df['auc'] = np.round(classification_accuracy_df['auc'], 2)\n",
    "    classification_accuracy_df.to_csv('outputs/analysis/classification_accuracy_%s_noncsf.csv' % version, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iamlab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
