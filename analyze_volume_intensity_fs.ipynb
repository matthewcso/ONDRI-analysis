{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_folder = 'outputs/metrics_ondri_fs/'\n",
    "metric_df = []\n",
    "seg_files = os.listdir(seg_folder)\n",
    "for file in seg_files:\n",
    "    metric_df.append(pd.read_csv(seg_folder + file))\n",
    "\n",
    "metric_df = pd.concat(metric_df)\n",
    "metric_df['SUBJECT'] = [x.split('.')[:-1][0] for x in seg_files]\n",
    "# if 'TBV-voxel' in metric_df.columns:\n",
    "#     metric_df = metric_df.drop(columns=['TBV-voxel'])\n",
    "\n",
    "metric_df = metric_df.dropna(axis=1, how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvox_cols = metric_df.columns[metric_df.columns.str.contains('nvox')]\n",
    "metric_df = metric_df.drop(columns=nvox_cols)\n",
    "#metric_df[nvox_cols] = np.asarray(metric_df[nvox_cols])/np.expand_dims(metric_df['TBV-voxel'], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_cols = metric_df.columns[metric_df.columns.str.contains('MII')  | metric_df.columns.str.contains('mean')]\n",
    "delete_cols = delete_cols + ['TBV-voxel']\n",
    "metric_df = metric_df.drop(delete_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_df = pd.read_csv('data/summary/ONDRI_summary.csv')\n",
    "clinical_df = clinical_df[~pd.isna(clinical_df['NII_FILENAME_T1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_t1_vols = pd.read_csv('outputs/fs_t1_volumes.csv')\n",
    "nvox_cols = fs_t1_vols.columns[~fs_t1_vols.columns.str.contains('SUBJECT')]\n",
    "fs_t1_vols[nvox_cols] = fs_t1_vols[nvox_cols].to_numpy()/np.expand_dims(fs_t1_vols['total_intracranial_volume'].to_numpy(), axis=-1)\n",
    "fs_t1_vols = fs_t1_vols.rename(columns={x: x + '_nvox' for x in nvox_cols})\n",
    "merged = pd.merge(clinical_df[['SUBJECT', 'COHORT']], metric_df, on='SUBJECT')\n",
    "\n",
    "merged = pd.merge(merged, fs_t1_vols, on='SUBJECT', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('outputs/analysis'):\n",
    "    os.makedirs('outputs/analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ADMCI', 'FTD')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('FTD', 'ALS')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ADMCI', 'PD')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ADMCI', 'FTD')\n",
      "('FTD', 'ALS')\n",
      "('ADMCI', 'PD')\n",
      "('ADMCI', 'FTD')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('FTD', 'ALS')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ADMCI', 'PD')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n"
     ]
    }
   ],
   "source": [
    "# train models\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import  recall_score, roc_auc_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix as confusion_matrix_fn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "import shap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for version in 'vol', 'texture', 'both':\n",
    "    comparisons = [('ADMCI', 'FTD'), ('FTD', 'ALS'), ('ADMCI', 'PD')]\n",
    "    classification_accuracy_df = []\n",
    "\n",
    "    for comparison in comparisons:\n",
    "        print(comparison)\n",
    "        labelmap = {comparison[0]: 0, comparison[1]: 1}\n",
    "\n",
    "        merged_select = merged[merged['COHORT'].isin(list(labelmap.keys()))]\n",
    "        features = merged_select[list(filter(lambda x: x not in [ 'SUBJECT', 'COHORT'], merged_select.columns))]\n",
    "        features = features.dropna(axis=1, how='any')\n",
    "  \n",
    "        if version == 'vol':\n",
    "            vol_cols = list(filter(lambda x: 'nvox' in x, features.columns))\n",
    "\n",
    "            features = features[vol_cols]\n",
    "\n",
    "        elif version == 'texture':\n",
    "            mii_cols = list(filter(lambda x: 'MAD' in x, features.columns)) \n",
    "            features = features[mii_cols]\n",
    "        #features = features.drop(columns=['TBV-voxel'])\n",
    "\n",
    "        labels = merged['COHORT']\n",
    "        labels = merged_select['COHORT'].map(labelmap)\n",
    "\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=49)\n",
    "\n",
    "        val_predictions_lr = np.zeros((labels.shape[0], 2))\n",
    "        val_predictions_rf = np.zeros((labels.shape[0], 2))\n",
    "        models_lr = []\n",
    "        shap_lr = []\n",
    "        models_rf =[]\n",
    "        shap_rf = []\n",
    "\n",
    "        for train_idx, val_idx in kf.split(features):\n",
    "            train_features = features.iloc[train_idx]\n",
    "            train_labels = labels.iloc[train_idx]\n",
    "            val_features = features.iloc[val_idx]\n",
    "            val_labels = labels.iloc[val_idx]\n",
    "\n",
    "            # ---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            # Random Forest training\n",
    "            model = BalancedRandomForestClassifier(random_state=0)\n",
    "            model.fit(train_features, train_labels)\n",
    "\n",
    "            explainer = shap.TreeExplainer(model, data=val_features)\n",
    "            shap_values = explainer.shap_values(val_features, check_additivity=False)\n",
    "\n",
    "            shap_rf.append(shap_values)\n",
    "            models_rf.append(model)\n",
    "            val_predictions_rf[val_idx] = model.predict_proba(val_features)\n",
    "\n",
    "\n",
    "        coef_df = pd.DataFrame()\n",
    "        coef_df['feature'] = features.columns\n",
    "        mean_imps = np.mean(np.array([model.feature_importances_ for model in models_rf]), axis=0)\n",
    "        mean_shaps = np.mean(np.abs(np.concatenate([shap_values[1] for shap_values in shap_rf])), axis=0)\n",
    "\n",
    "        coef_df['shap'] = mean_shaps\n",
    "        coef_df['imps'] = mean_imps\n",
    "        coef_df['mean_percent'] = [(np.mean(features[x][labels==1])/np.mean(features[x][labels==0])) for x in coef_df['feature']]\n",
    "        coef_df['p'] = [ttest_ind(features[x][labels==0], features[x][labels==1]).pvalue for x in coef_df['feature']]\n",
    "        coef_df = coef_df.sort_values(by='shap', ascending=False)\n",
    "        coef_df.to_csv('outputs/analysis/fs_%s_vs_%s_%s.csv' % (comparison[0], comparison[1], version), index=False)\n",
    "\n",
    "\n",
    "        confusion_matrix = confusion_matrix_fn(labels, np.argmax(val_predictions_rf, axis=-1))\n",
    "\n",
    "        recall_0 = confusion_matrix[0, 0]/ (confusion_matrix[0, 0] + confusion_matrix[0, 1]) # (yes this agrees with sklearn confusion_matrix)\n",
    "        recall_1 = confusion_matrix[1, 1]/ (confusion_matrix[1, 0] + confusion_matrix[1, 1])\n",
    "\n",
    "        classification_accuracy_df.append(pd.Series({'comparison': '%s vs %s' % (comparison[0], comparison[1]), \n",
    "                                                    'auc': roc_auc_score(labels, val_predictions_rf[:, 1]),\n",
    "                                                    'Recall 0': recall_0,\n",
    "                                                    'Recall 1': recall_1,\n",
    "                                                    })) \n",
    "                                                    \n",
    "                                                                    \n",
    "    classification_accuracy_df = pd.DataFrame(classification_accuracy_df)\n",
    "\n",
    "    classification_accuracy_df['Recall 0'] = np.round(classification_accuracy_df['Recall 0'], 2)\n",
    "    classification_accuracy_df['Recall 1'] = np.round(classification_accuracy_df['Recall 1'], 2)\n",
    "    classification_accuracy_df['auc'] = np.round(classification_accuracy_df['auc'], 2)\n",
    "    classification_accuracy_df.to_csv('outputs/analysis/classification_accuracy_fs_%s.csv' % version, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iamlab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
